{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72b09740-9874-42e9-8731-7b5f2290620c",
      "metadata": {
        "id": "72b09740-9874-42e9-8731-7b5f2290620c"
      },
      "source": [
        "# Loan Default Prediction\n",
        "\n",
        "**Project Overview**\n",
        "\n",
        "Build a predictive model that assigns default probabilities to loan applications.\n",
        "Minimize nancial risk by accurately predicting the likelihood of loan defaults, enabling\n",
        "more informed and strategic lending decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08de959-f2f8-4d01-8a79-ca1100cde3b4",
      "metadata": {
        "id": "f08de959-f2f8-4d01-8a79-ca1100cde3b4"
      },
      "source": [
        "# Notebook Structure\n",
        "---\n",
        "<details>\n",
        "<summary><b>1. Business Problem and Objectives</b></summary>\n",
        "   Define the problem being addressed and its relevance to real-world scenarios.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>2. Data Acquisition and Preparation</b></summary>\n",
        "\n",
        "- ### **2.1 Data Source and Download**  \n",
        "  Explanation of the dataset source and how it was obtained.  \n",
        "\n",
        "- ### **2.2 Installing Required Modules**  \n",
        "  List and install the libraries needed for the project.  \n",
        "\n",
        "- ### **2.3 Importing Modules and Global Variables**  \n",
        "  Set up imports and define constants or global variables.  \n",
        "\n",
        "- ### **2.4 Defining Supplemental Functions**  \n",
        "  Helper functions to streamline data processing.  \n",
        "\n",
        "- ### **2.5 Data Loading**  \n",
        "  Load the dataset into a DataFrame or suitable data structure.  \n",
        "\n",
        "- ### **2.6 Basic Data Understanding**  \n",
        "  Perform initial data exploration, including shape, columns, and types.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>3. Data Preprocessing and Feature Engineering</b></summary>\n",
        "\n",
        "- ### **3.1 Cleaning**  \n",
        "\n",
        "- ### **3.2 Preprocessing**  \n",
        "\n",
        "- ### **3.3 Feature Extraction**  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>4. Predictive Analysis</b></summary>\n",
        "\n",
        "- ### **4.1 Train-Test Data Split**  \n",
        "- ### **4.2 Classification with Simple Model**   \n",
        "  Choose a simple base classification model and train it on the preprocessed data. Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.3 Selecting Best Model for Feature Reduction**\n",
        "  Deploy several advanced classification models with feature interpretability.\n",
        "- ### **4.4 Feature Reduction Using Best Advanced Model**\n",
        "  Reducing dataset to most important features from best performing model.        \n",
        "- ### **4.5 Tuning Hyperparameters for Logistic Regression Model**\n",
        "  Use parameter grid search to find the best-performing model.  \n",
        "- ### **4.6 Improved Model Performance**\n",
        "  Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.7 Feature Interpretation**\n",
        "  Visualize results and discuss findings, including strengths and limitations.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>5. Conclusion</b></summary>\n",
        "Summarize work.\n",
        "Summarize findings, including strengths and limitations.\n",
        "Suggest future work.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2836a232-89aa-4a22-9205-72dc03169293",
      "metadata": {
        "id": "2836a232-89aa-4a22-9205-72dc03169293"
      },
      "source": [
        "## 1.1 Business Problem and Objectives\n",
        "\n",
        "**Problem Statement:**\n",
        "**Key Questions:**\n",
        "**Project Objectives:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3828a3f4-ed95-4a95-899a-284a5a62c756",
      "metadata": {
        "id": "3828a3f4-ed95-4a95-899a-284a5a62c756"
      },
      "source": [
        "# 2. Data Acquisition and Preparation\n",
        "\n",
        "## 2.1. Data Understanding\n",
        "\n",
        "This section outlines the source of the data used in this project,  and provides instructions for downloading it.\n",
        "\n",
        "**Data Sources**\n",
        "**Data Relevance**\n",
        "**Data Limitations**\n",
        "**Download Instructions**\n",
        "**Data Storage**\n",
        "**Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "259af49e-e19b-44dd-8cd3-8d0ea4b91298",
      "metadata": {
        "id": "259af49e-e19b-44dd-8cd3-8d0ea4b91298"
      },
      "source": [
        "## 2.2 Installing Required Modules\n",
        "\n",
        "This section focuses on installing the necessary Python libraries and packages required\n",
        "\n",
        "1. **Requirements File**\n",
        "    - We retrieve the list of required packages from a `requirements.txt` file hosted on GitHub using `wget`. This file contains the names and versions of all the dependencies.\n",
        "    - This ensures that we install the correct versions of the libraries for compatibility and reproducibility.\n",
        "2. **Installation using pip**\n",
        "    - We use Python's `pip` package manager to install the libraries listed in the `requirements.txt` file.\n",
        "    - The `-r` flag instructs `pip` to read the requirements file and install all the packages listed within."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6fdb28-b32a-42fc-96d6-5b91152aaa29",
      "metadata": {
        "id": "0a6fdb28-b32a-42fc-96d6-5b91152aaa29"
      },
      "source": [
        "Obtaining Data from the [Loan Default Prediction Competition](https://www.kaggle.com/competitions/home-credit-default-risk/overview) on **Kaggle:**\n",
        "\n",
        "Before accessing the loan default prediction data, you must first join the competition and agree to its specific Terms & Conditions. Follow the step:\n",
        "\n",
        "1. Create/Log In to Your Kaggle Account.\n",
        "2. Navigate to the Competition Page. This page will have all the relevant details about the competition, including the rules and guidelines.\n",
        "3. Click the “Join Competition” button. This action will prompt you to review and agree to the competition's Terms & Conditions. You must accept these terms before you can access the data.\n",
        "4. Downloading the Dataset.\n",
        "  \n",
        "  a) **Manual Download**. After joining and agreeing to the terms, use the download button provided on the competition page to download the dataset directly to your computer.\n",
        "\n",
        "  b) **Using the Kaggle API**. If you prefer using the command line, install the Kaggle API.\n",
        "\n",
        "  `!pip install kaggle`\n",
        "\n",
        "  Next, ensure your Kaggle API credentials (found in your account settings) are correctly set up (typically by placing the kaggle.json file in the ~/.kaggle/ directory). Then, run:\n",
        "\n",
        "  `!kaggle competitions download -c home-credit-default-risk`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json file here\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Lg30jQrgOtou",
        "outputId": "c0509ce6-adc1-4121-865b-c07464909ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "id": "Lg30jQrgOtou",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d876e7ad-3a29-47da-bea9-de3520ff7dd6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d876e7ad-3a29-47da-bea9-de3520ff7dd6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code will download the dataset into either `/content` if environment is `Colab`, or into local `Downloads` folder."
      ],
      "metadata": {
        "id": "HiXGTkraPjCm"
      },
      "id": "HiXGTkraPjCm"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ac1d2134-685f-44ec-90a2-a295bedbc582",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac1d2134-685f-44ec-90a2-a295bedbc582",
        "outputId": "d3fa400f-4f5b-430a-ce05-07a5fe280614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded successfully!\n",
            "Downloading home-credit-default-risk.zip to /content\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset !kaggle competitions download -c home-credit-default-risk\n",
        "# A fancier way of doing the kaggle download with try-catch block, if notebook is executed locally\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # Run the Kaggle dataset download command\n",
        "    result = subprocess.run(\n",
        "        [\"kaggle\", \"competitions\", \"download\", \"-c\", \"home-credit-default-risk\"],\n",
        "        check=True,  # Raise an exception if the command fails\n",
        "        text=True,   # Capture output as text\n",
        "        capture_output=True  # Capture stdout and stderr\n",
        "    )\n",
        "    print(\"Dataset downloaded successfully!\")\n",
        "    print(result.stdout)  # Print the command output\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Error occurred while downloading the dataset.\")\n",
        "    print(f\"Return code: {e.returncode}\")\n",
        "    print(f\"Error output: {e.stderr}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Kaggle CLI is not installed. Please install it and ensure it's in your PATH.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code contains supplemental functions that will extra ct and move datasets into defailt `data` directory.  "
      ],
      "metadata": {
        "id": "pUaIqAIuP62O"
      },
      "id": "pUaIqAIuP62O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Function checks if directory exists\n",
        "def ensure_directory(path):\n",
        "    \"\"\"\n",
        "    Ensure that a directory exists. If not, create it.\n",
        "    \"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Directory ensured: {path}\")\n",
        "\n",
        "# Function downloads files\n",
        "def download_files(base_url, file_names, destination_dir):\n",
        "    \"\"\"\n",
        "    Download a list of files from a base URL to a specified directory.\n",
        "\n",
        "    Args:\n",
        "    - base_url (str): The base URL for the files.\n",
        "    - file_names (list): List of filenames to download.\n",
        "    - destination_dir (str): Directory to save the downloaded files.\n",
        "    \"\"\"\n",
        "    for file_name in file_names:\n",
        "        url = f\"{base_url}/{file_name}\"\n",
        "        dest_path = os.path.join(destination_dir, file_name)\n",
        "        if not os.path.exists(dest_path):\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded: {file_name}\")\n",
        "        else:\n",
        "            print(f\"File already exists: {file_name}\")\n",
        "\n",
        "# Function unzips archive into directory\n",
        "def unzip_dataset(zip_path, destination_dir):\n",
        "    \"\"\"\n",
        "    Unzip a dataset into the specified directory.\n",
        "\n",
        "    Args:\n",
        "    - zip_path (str): Path to the zip file.\n",
        "    - destination_dir (str): Directory to extract the zip contents.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_dir)\n",
        "    print(f\"Unzipped: {zip_path} to {destination_dir}\")\n",
        "\n",
        "# Function to determine data directory, depending on runtime environment.\n",
        "\n",
        "def determine_data_dir():\n",
        "    \"\"\"\n",
        "    Determines the data directory based on the execution environment:\n",
        "    - Local: Uses 'Data' directory in the current working directory.\n",
        "    - Cloud (e.g., Google Colab): Uses '/content' as the data directory.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the appropriate data directory.\n",
        "    \"\"\"\n",
        "    if 'COLAB_GPU' in os.environ:  # Check if running in Google Colab\n",
        "        data_dir = \"/content/data\"\n",
        "        print(f\"Running in Google Colab. Using data directory: {data_dir}\")\n",
        "    else:\n",
        "        data_dir = os.path.join(os.getcwd(), \"data\")\n",
        "        print(f\"Running locally. Using data directory: {data_dir}\")\n",
        "\n",
        "        # Ensure the 'data' directory exists locally\n",
        "        if not os.path.isdir(data_dir):\n",
        "            print(f\"The directory '{data_dir}' does not exist. Please create it and place the data files there.\")\n",
        "            raise FileNotFoundError(f\"'{data_dir}' directory is required for local execution.\")\n",
        "\n",
        "    return data_dir"
      ],
      "metadata": {
        "id": "bGsvdbSPP5rQ"
      },
      "id": "bGsvdbSPP5rQ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code syncs repository structure with colab or local version."
      ],
      "metadata": {
        "id": "irop-8__QPtl"
      },
      "id": "irop-8__QPtl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data directories\n",
        "data_dir = determine_data_dir()\n",
        "\n",
        "# Get the parent directory of data_dir\n",
        "base_dir = os.path.dirname(data_dir)\n",
        "models_dir = os.path.join(base_dir, \"models\")\n",
        "images_dir = os.path.join(base_dir, \"images\")\n",
        "\n",
        "# Ensure directories exist\n",
        "ensure_directory(data_dir)\n",
        "ensure_directory(models_dir)\n",
        "ensure_directory(images_dir)\n",
        "\n",
        "# Dataset path\n",
        "zip_file_path_dataset = os.path.join(\"home-credit-default-risk.zip\")\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(zip_file_path_dataset):\n",
        "    print(\"File found. Proceeding to unzip...\")\n",
        "    # Unzip dataset\n",
        "    unzip_dataset(\"home-credit-default-risk.zip\", data_dir)\n",
        "    # Remove after unzipping\n",
        "    os.remove(zip_file_path_dataset)\n",
        "    print(f\"Removed ZIP file: {zip_file_path_dataset}\")\n",
        "else:\n",
        "    print(\"File not found. Please check the path or download the Dataset from Kaggle.\")\n",
        "\n",
        "# Download supplemental data\n",
        "github_base_url = \"https://raw.githubusercontent.com/leksea/loan-default-prediction/main/data\"\n",
        "supplemental_files = [\n",
        "]\n",
        "download_files(github_base_url, supplemental_files, data_dir)\n",
        "\n",
        "# Download model into models directory\n",
        "model_base_url = \"https://raw.githubusercontent.com/leksea/loan-default-prediction/main/models\"\n",
        "model_files = [\n",
        "]\n",
        "download_files(model_base_url, model_files, models_dir)\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "id": "cnXkp1_hQJTG",
        "outputId": "fbf4faff-87d0-4a30-9f03-f131caddb1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cnXkp1_hQJTG",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/data\n",
            "Directory ensured: /content/data\n",
            "Directory ensured: /content/models\n",
            "Directory ensured: /content/images\n",
            "File found. Proceeding to unzip...\n",
            "Unzipped: home-credit-default-risk.zip to /content/data\n",
            "Removed ZIP file: home-credit-default-risk.zip\n",
            "Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Importing Modules and Global Variables"
      ],
      "metadata": {
        "id": "T2WlvK3LAd7W"
      },
      "id": "T2WlvK3LAd7W"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "98debd5b-dbb3-4199-afe7-76dfa0a260c3",
      "metadata": {
        "id": "98debd5b-dbb3-4199-afe7-76dfa0a260c3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Defining Supplemental Functions"
      ],
      "metadata": {
        "id": "7djSu9J9AigW"
      },
      "id": "7djSu9J9AigW"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "peNnEX8QA12v"
      },
      "id": "peNnEX8QA12v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 Data Loading"
      ],
      "metadata": {
        "id": "t_XyP8EKA1Xu"
      },
      "id": "t_XyP8EKA1Xu"
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the files\n",
        "# determine the data directory\n",
        "data_dir = determine_data_dir()\n",
        "\n",
        "df_col_desc = pd.read_csv(os.path.join(data_dir, 'HomeCredit_columns_description.csv'), encoding='latin1')\n",
        "df_application_train = pd.read_csv(os.path.join(data_dir, 'application_train.csv'), encoding='latin1')\n",
        "df_application_test = pd.read_csv(os.path.join(data_dir, 'application_test.csv'), encoding='latin1')\n",
        "df_bureau = pd.read_csv(os.path.join(data_dir, 'bureau.csv'), encoding='latin1')\n",
        "df_bureau_balance = pd.read_csv(os.path.join(data_dir, 'bureau_balance.csv'), encoding='latin1')\n",
        "df_credit_card_balance = pd.read_csv(os.path.join(data_dir, 'credit_card_balance.csv'), encoding='latin1')\n",
        "df_installments_payments = pd.read_csv(os.path.join(data_dir, 'installments_payments.csv'), encoding='latin1')\n",
        "df_POS_CASH_balance = pd.read_csv(os.path.join(data_dir, 'POS_CASH_balance.csv'), encoding='latin1')\n",
        "df_previous_application = pd.read_csv(os.path.join(data_dir, 'previous_application.csv'), encoding='latin1')\n",
        "df_application_test = pd.read_csv(os.path.join(data_dir, 'application_test.csv'), encoding='latin1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5UXYd8MA357",
        "outputId": "d9e0c7dd-7045-44c2-a60a-a1da974ee596"
      },
      "id": "h5UXYd8MA357",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.6 Basic Data Understanding\n",
        "\n",
        "Running built-in functions to gain insights about the data frames."
      ],
      "metadata": {
        "id": "2rsAAP2iA48F"
      },
      "id": "2rsAAP2iA48F"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7znT1_scA9lv"
      },
      "id": "7znT1_scA9lv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}